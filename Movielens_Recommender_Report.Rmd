---
title: "Data Science Capstone - Movielens Recommendation Model"
author: "Carlos Y치침ez Santib치침ez"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document:
    number_sections: yes
    toc: yes
    fig_caption: yes
    includes:
      in_header: header_includes.tex
geometry: margin=1in,headheight=70pt,headsep=0.3in
linkcolor: blue
mainfont: Arial
fontsize: 11pt
always_allow_html: true
#fig_width: 1
#fig_height: 1 
---

 <!-- To compile the report into pdf, the files header_includes.tex is require. This file is available at -->
 <!-- https://github.com/carlosyanez/Movielens-10-M-Recommendation/blob/master/header_includes.tex -->
 <!-- In addition, for faster generation, datasets have been saved into movielens.RDate available at  -->
 <!-- https://drive.google.com/open?id=1yrZ4zMqOIawRjUuLXcJz10YcBM8NnSiS  -->


```{r, setup, include=FALSE}
#### WARNING : Running the notebook from scratch, may take SEVERAL HOURS - Run at your own time.####

# Avoid to run time consuming analysis - comment if you want to run the notebook from scratch

knitr::opts_chunk$set(echo=FALSE,eval=FALSE,message=FALSE, warning=FALSE,tidy=FALSE,fig.align="center",
                      fig.width=5, fig.height=3)

# Load environment with saved dataset for faster knitting - comment if you want to run the notebook from scratch
# you can download the file from https://drive.google.com/open?id=1yrZ4zMqOIawRjUuLXcJz10YcBM8NnSiS

load("movielens.RData") 

# Load functions created for this excercise
source("hybrid_recommender.R", echo = F, prompt.echo = "", spaced = F)


if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(gtools)) install.packages("gtools", repos = "http://cran.us.r-project.org")
if(!require(ClusterR)) install.packages("ClusterR", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org")

knitr::knit_hooks$set(inline = function(x) { if(!is.numeric(x)){ x }else{ prettyNum(round(x,3), big.mark=" ") } })
target_rmse <- 0.86490

```

```{r Load_Movielens}

## Uncomment the below lines to reload the Movielens database
#movielens_10M<-Movielens_Data_Loader()
#edx <- movielens_10M$edx
#validation <- movielens_10M$validation
#rm(movielens_10M)
```
\newpage
# Introduction
 <!-- introduction/overview/executive summary section that describes the dataset and summarizes the goal of the project and key steps that were performed -->

This document presents a machine learning model, aiming to predict (recommend) movie ratings. This report is a capstone assignment for HarvardX's Professional Certificate in Data Science, which can be taken at the edX platform. The program is available at   [https://www.edx.org/professional-certificate/harvardx-data-science]( https://www.edx.org/professional-certificate/harvardx-data-science).

The data used in this exercise comes from  the [Movielens 10M Dataset](https://grouplens.org/datasets/movielens/10m/). After using the download code provided in the course, the resulting data frame contains a series of  observations, each with the below attributes:

1. **`r colnames(edx)[1]`** : Unique user identifier
2. **`r colnames(edx)[2]`** : Unique movie identifier
3. **`r colnames(edx)[3]`** : Rating given to this movie by the particular user.
4. **`r colnames(edx)[4]`** : Timestamp indicating when the the user submitted the rating.
5. **`r colnames(edx)[5]`** : Title of the film and its release year in brackets. Please note that different movies can have the same name (e.g. remakes), thus `r colnames(edx)[2]` is a better unique identifier.
6. **`r colnames(edx)[6]`** : List of all genres, in which this movie can be classified.

```{r Column_Names}
#Used to retrieve column names during report writing
colnames(edx)
```

The goal of this project is to generate a model that can predict a particular movie rating, as close as possible to the actual rating given to each film. In order to assess performance, the **Root Square Mean Error (RMSE)** will be calculated, with the error defined as the diference between predicted and actual ratings. The Training and validation datasets are generated using the code provided in the assignment instructions. In order to aim for a full mark, this report will target for a RMSE lower than **`r sprintf("%3.5f", target_rmse)`**.

The starting point of this project is the model presented in section [33.7 of the course's textbook](https://rafalab.github.io/dsbook/large-datasets.html#recommendation-systems). Then, the following steps are presented:

1. Analysis of the textbook's model and possible ways to improve it.
2. Improvement adding user clusters.
3. Tuning improved model.
4. Evaluation against validation dataset.
5. Conclusion.

The subsequent sections present the above points in further detail.

This file and its associated R file are available in [Github](https://github.com/carlosyanez/Movielens-10-M-Recommendation).


\newpage
# Methods and Analysis
<!-- methods/analysis section that explains the process and techniques used, including data cleaning, data exploration and visualization, insights gained, and your modeling approach -->

## Preparing the data

Before conducting any modelling, the data needs to be cleaned up a little bit, and then split into training and testing set.

In terms of data cleaning, three operations have been considered, namely:

* Remove the year from the title and store it in a different column. This may be useful for modelling.
* For the same reasons, convert the time stamp into a year number.
* Finally, add a sequential number (*row_id*). This will be create a single unique ID for each record (instead of a userId and movieId combination) and maybe useful to speed up filtering, given the large size of the dataset.

To achieve this, the function **Tidy_Up** has been created. The code for this function is available on the included R file. Then, training and test sets are created with the below code:


```{r Tidy_Up_Data, echo=TRUE}
#tidy up data 
edx_tidy_temp <- Tidy_Up(edx)

#divide Train and Test datasets
version_number<-as.double(version$minor)/10+as.double(version$major)
if(version_number<3.6){
  set.seed(200)
}else{
    set.seed(200, sample.kind="Rounding")
}

edx_train_test <- Train_Test(edx_tidy_temp)

#remove temporary and original dataset to avoid filling up memory.
rm("edx_tidy_temp","edx")

```

This is a sample of the generated training set (*edx_train_test$train*):

```{r sample_training_dataset, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
edx_train_test$train[1:10,] %>% kable(caption="Training Dataset - Sample",format = "latex", booktabs = TRUE) %>%  kable_styling(latex_options = c("hold_position","scale_down"))
```

## Textbook Model

As a starting point, the model presented in [section 33.7 of the course's textbook](https://rafalab.github.io/dsbook/large-datasets.html#recommendation-systems) will be used. This model estimates rating by assuming that a particular rating can be seen as a combination of the average rating for all movies (the *true* rating) plus movie and user effects (biases). In order to account for cases where movies and users don't have many reviews against them, weighting parameters have been added. All of this can be expressed by the below equation:

$$\large{predicted \ rating =  \hat{\mu} + b_i + b_u}$$


where $\hat(\mu)$ is the average rating for all movies in the dataset, and $b_i$ and $b_u$ are the movie bias and user bias (respectively), both defined as follows:

$$b_i = \frac{1}{\lambda_1 + m}\sum_{j=1}^{m} (rating_j - \hat{\mu})$$
$$b_u = \frac{1}{\lambda_2 + n}\sum_{j=1}^{n} (rating_j - \hat{\mu} - b_{i,j})$$

where

* $rating_j$ : is a particular movie rating 
* $m$ : is the number of all ratings for a particular movie
* $n$ : is the total number of ratings submitted from a particular user (one rating per movie).
* $ b_{i,j}$ : is the *movie bias* for a particular movie
* $\lambda_1$ :  is a weighting parameter for *movie biases*.
* $\lambda_2$ :  is a weighting parameter for *user biases*.

The calculation of these parameters has been written into the function *General_Biases*. To create the first prediction, $lamdba$ parameters are set to zero. The results are evaluated using the provided *RMSE* function.

```{r Calculate_Biases, echo=TRUE, eval=TRUE}
# create biases

general_biases <- General_Biases(edx_train_test$train)

# join with dataset and predict

data_set <- edx_train_test$test  %>%
  left_join(general_biases$movie_avgs, by = "movieId") %>%
  left_join(general_biases$user_avgs, by = "userId")

first_prediction <- data_set %>%
    mutate(predicted_rating = general_biases$mu_hat +
             b_i + b_u, error = rating - predicted_rating) %>%
    filter(!(predicted_rating == general_biases$mu_hat))

# Calculate RMSE and put it in comparison table
rmse_value <- RMSE(first_prediction$rating,
                   first_prediction$predicted_rating)
compare_RMSE <- tibble(Model=character(),
                       RMSE = double(),
                       "Below Target"=logical())
compare_RMSE <- add_row(compare_RMSE,
                        Model="Textbook",
                        RMSE=rmse_value,
                        "Below Target"=(rmse_value<target_rmse))

# clean up 
general_biases_first <- general_biases
rm(first_prediction,data_set,rmse_value)
```

```{r first_Prediction, eval=TRUE}

compare_RMSE %>% kable(caption="First Prediction",format = "latex", booktabs = TRUE) %>%  kable_styling(latex_options = c("hold_position"),position = "center")
```

As expected, the RMSE is higher than the target value. After considering the possible causes for this result, part of this may be related to the fact that this model is based on averages: although this is not necessarily a bad approach and works for many cases, it also has some limitations, namely:

* it won't perform well with *divisive* movies. For example a niche film will obtain 5 stars from the fans, but will be many users which will dislike. This won't be reflected by an average bias (and the user bias may not necessarily compensate this).
* There will be users with extreme ratings (i.e. who absolutely like family movies and completely dislike horror films). A user bias average will fail to reflect this situation.
* Already mentioned in the textbook, when the number of reviews for a user or a film is low, this model won't predict accurately.

Taking all those points into account, the *General_Biases* function has been implementes to also calculate the standard deviation of the movie and user biases (named *s_i* and *s_u* respectively). The below charts show their results:

```{r Movie_Biases_1, eval=TRUE}
 general_biases_first$movie_avgs %>% filter(!is.na(s_i)) %>%
  mutate(s_i=round(s_i,5)) %>% ggplot(aes(s_i)) +
  geom_histogram(binwidth=0.01,color='navy',fill='navy') +
  geom_vline(xintercept=1, linetype="dashed", color = "red",size=0.8) + theme_grey()+
  labs(title="Movie biases deviations",x="s_i", y = "Movies")
```

```{r User_Biases, eval=TRUE}
 general_biases_first$user_avgs %>% filter(!is.na(s_u)) %>%
  mutate(s_u=round(s_u,5)) %>% ggplot(aes(s_u)) +
  geom_histogram(binwidth=0.01,color='navy',fill='navy') +
  geom_vline(xintercept=1, linetype="dashed", color = "red",size=0.8) + theme_grey() +
  labs(title="User biases deviations",x="s_u", y = "Movies")
```

As shown, there is a significant percentage of movie (`r round(mean(general_biases_first$movie_avgs %>% filter(!is.na(s_i)) %>% .$s_i >=1)*100,2)` %) and user (`r  round(mean(general_biases_first$user_avgs %>% filter(!is.na(s_u)) %>% .$s_u >=1)*100,2)` %) biases equal or larger than 1. This means there will be many cases where this model with values contributing to a larger RMSE. Although movies/users with lower number of reviews have higher deviations, this is not always the case, as illustrated in the below graph.

```{r deviations_per_reviews, eval=TRUE}
general_biases_first$movie_avgs %>% filter(!is.na(s_i)) %>% mutate(s_i=round(s_i,3)) %>% group_by(n_i,s_i) %>%
  summarise(size_i=n()) %>%
  ggplot(aes(x=n_i,y=s_i,size=size_i)) + geom_point(color='navy',fill='navy') +  
  labs(title="Deviation per number of reviews",x="Reviews per movie", y = "s_i", size="Number of movies") + 
  geom_hline(yintercept=1, linetype="dashed", color = "red",size=0.8) +
  theme_grey() + scale_x_continuous(trans='log10') 
```

Perhaps, a way to illustrate this is to look at two films with similar number of reviews and average rating (and close $b_i$) but different standard deviations for their ratings.

```{r Deviation Comparison,eval=TRUE}
# Romeo + Juliet
selected_reviews_1<- edx_train_test$train %>% filter(movieId==1059) 
selected_title_1 <- unique(selected_reviews_1$title)
selected_1 <- selected_reviews_1%>%  ggplot(aes(rating))  + geom_histogram(bins = 10,color='navy',fill='navy') +  labs(title=selected_title_1,x="Rating", y = "Reviews")  + theme_grey()

# The Bourne Supremacy
selected_reviews_2<- edx_train_test$train %>% filter(movieId==8665) 
selected_title_2 <- unique(selected_reviews_2$title)
selected_2 <- selected_reviews_2%>%  ggplot(aes(rating))  + geom_histogram(bins = 10,color='navy',fill='navy') +  labs(title=selected_title_2,x="Rating", y = "Reviews")  + theme_grey()

#plot
grid.arrange(selected_1, selected_2, nrow=2)

#cleanup
rm(selected_reviews_1,selected_title_1,selected_1,selected_reviews_2,selected_title_2,selected_2)

common_users <- nrow(edx_train_test$train %>% filter(movieId %in% c(8665,1059)) %>%  select(userId) %>% unique())
```

Looking at the charts above, it looks like *Romeo+Juliet* has a wider range of opinions, compared to a more concentrated range of ratings in the case of  *The Bourne Supremacy*. This means the model won't be as effective for this two films in the cases for the `r common_users` users who have rated both films and may be in all sections of liking/disliking this particular combination of films.

## An Improved Model : User Clustering

All the previous observations notwithstanding, it is still worth pointing out that the above model still has merit, given its relative simplicity and ease of calculation. 

Perhaps, 8 million reviews is a large number of reviews and it is possible to generate ratings based on smaller groups without loss of accurary or without invalidating the averages approach. Perhalps, if data is segmented in groups with similar traits, the overall error may decrease.

One way to achieve this is to group the users with similar movie preferences (i.e split the ones who like Shakespearean romances from the fans of action thrillers). In this excercise, the chosen was to undertake this task is to leverage the genres of the already rated films to create distinct user vectors or *profiles*.

```{r Create_User_Vector}
user_vector <-User_Vectoriser(edx_train_test$train)
```

As mentioned before, each film has a set of genres they belong (from one to many). Collectively, there are `r nrow(user_vector$genres)` genres, listed below.

```{r genres_list, eval=TRUE}
user_vector$genres %>% kable(caption="Genres in training set, ordered by occurrences",format = "latex", booktabs = TRUE,format.args = list(decimal.mark = '.', big.mark = " ")) %>%  kable_styling(latex_options = c("HOLD_position"))

user_vector$genres %>% kable(caption="Genres in training set, ordered by occurrences",format = "html", booktabs = TRUE,format.args = list(decimal.mark = '.', big.mark = " ")) 
```

In order to profile each user, this report has used the genre property in two ways:

* The average rating each user gives to each genre, for each film rated (i.e. how much each user likes each genre).
* The percentage of each genre compared to the overall reviews per user (i.e. how much each user watches each genre).

This will result in a *user_vector*, for each user in the training set. The algorithm has been written in the *User_Vectoriser* function, available in the project's R file. The below table shows a sample of the user vectors created with this code.

```{r User_Vector_Sample_1, eval=TRUE}

user_vector$user_averages[1:10,] %>% mutate_if(is.numeric, format, digits=4,nsmall = 0) %>%
  kable(caption="User Vectors - Averages",format = "latex", booktabs = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position","scale_down"))
```
```{r User_Vector_Sample_2, eval=TRUE}
user_vector$user_weights[1:10,]  %>% mutate_if(is.numeric, format, digits=4,nsmall = 0) %>%
  kable(caption="User Vectors - Percentages",format = "latex", booktabs = TRUE) %>% 
  kable_styling(latex_options = c("HOLD_position","scale_down"))
```


The average number of genres per user is `r mean(rowSums(user_vector$user_weights[,2:ncol(user_vector$user_weights)]>0))`. 

```{r Genres_per_USer, eval=TRUE}
tibble(genres=rowSums(user_vector$user_weights[,2:ncol(user_vector$user_weights)]>0)) %>% 
  ggplot(aes(genres)) +
  geom_histogram(bins=20,color='navy',fill='navy') + theme_grey() +
  labs(title="Distribution of Number of Genres per User",x="Genres", y = "Users")

```


Using the vectors it is possible to leverage existing clustering algorithms such as k-means (presented in [section 34 of the textbook](https://rafalab.github.io/dsbook/clustering.html)). However after an initial test other methods where considered and  **Gaussian Mixture Model clustering** (GMM) was chosen instead. [This link](https://towardsdatascience.com/gaussian-mixture-models-d13a5e915c8e) offers and explanation of the this method and the reasons why can be a better way in this particular case. In this exercise, the GMM implementation in [**ClusterR**](https://cran.r-project.org/web/packages/ClusterR/ClusterR.pdf) package is used. The code below shows a sample of how GMM is used in this project.


```{r GMM sample,echo=TRUE}
#First we need to combine weights and averages, and remove the first columns
user_profiles <- user_vector$user_averages %>%
      left_join(user_vector$user_weights, by = "userId")
us <- user_profiles %>% select(-userId) 

## Then we need to specified the number of clusters
## we want the users to be classified into, eg 5
cluster_n <- 5

###The, we run GMM as per the manual
fit <- GMM(us, cluster_n, dist_mode = "maha_dist", 
           seed_mode = "random_subset", km_iter = 10,
           em_iter = 10, verbose = F)

pr <- predict_GMM(us, fit$centroids, fit$covariance_matrices, fit$weights)
user_profiles$group <- pr$cluster_labels

###Show user profiles

user_profiles
```

Before running the above code, it is worth noticing that:

* The combined average and weight vectors have 40 dimensions. In most cases, a vector only has present on subset of those dimensions (in this particular training set, none has). Potentially there are  vectors without common touchpoints (action&adventure fans opposite documentaries&film-noir followers).
* Although no analysis has been done for this report, it is reasonable to assume that some genres have a big overlap. Hence, it is reasonable to assume that  not all dimensions are *independent variables*.
* Based on the previous numbers, it is also reasonable to assume that a large percentage of users can be group using only  a subset of the most popular genres.
* Taking this into account, using all dimensions may not be the best method of clustering - and it is interesting to assess how this impacts the RMSE.
* In addition, the number of clusters  - which will certainly have an impact on the RMSE - is an input parameter.

Considering all the points above and after some initial tests, this document proposes a clustering algorithm based on GMM which follows the below steps:

1. User vector is created.
2. The number of genres used for clustering (*n*) is set.
3. Using the genre percentages, the relative weight of the *n* most popular genres is calculated.
4. The user vector is filtered to only keep the cases where the previously calculated relative weight meets/exceeds a threshold; This filters out the users for which the selected genres are not relevant.
5. The filtered vector is fed into the GMM functions and users are group into an initial set of clusters.
6. The remaining users are sorted, and filtered by their *n* most popular genres. Again, they are filtered by the predefined threshold and fit into a new set of clusters.
7. Step 6 repeated as many times as desired or until there are either no users left or there is no enough data to cluster them.
8. All remaining users (if any) and classified in a residual group.

This algorithm as been written into the **User_Classifier** function, available in the R file. It is worth noticing this process has four *arbitrary* parameters that will change the number of resulting clusters and therefore the may have an impact on the RMSE - this will be discussed later in the document.

Once the users have been segmented, it is possible to use the *textbook* model to calculate the average and biases for each of these groups and then predict ratings (the objective of this entire process!). The first part of this step is straightforward (just add the group to the dataset and use this parameter to group prior to summarising) and it has been written into a function called **Group_Biases**.

However, it needs to be considered that potentially some user weren't not grouped into a meaningful cluster and thus this method makes no sense. Additionally, there maybe situations where a particular group or movie is absent from the training data and this method is not valid. To cover those situations, the prediction algorithm has two steps:

1. A prediction method using group averages and biases for all qualifying reviews - just like the textbook model but split per group.
2. A *backstop* method for all reviews don't fit the above - for this the 'textbook' method is used.

Please note that both segments have tuning parameters ($\lambda_{1-4}$).

This prediction algorithm has been written into a function called **Rating_Predicter**.

## Tuning Step 1 - Optimise Clustering.

The first optimisation step consists in finding the optimal clustering parameters to minimise the RMSE. These parameters are:

1. the number of genres (*genres*) to consider in each clustering round.
2. the number of target clusters per round (*cluster_n*).
3. the cut-off value to filter the users for each clustering round (*cut-off*).
4. the number of time clustering will be attempted (*iterations*).

This process has been executed through the below code.

```{r Cluster_Optimisation, echo=TRUE}

#create tibble to record stats

stats <- tibble(clustering=character(),iterations=double(),cutoff=double(),
                genres=double(),cluster_n=double(),RMSE = double(),
                RMSE_1 = double(),RMSE_2 = double(),
                RMSE_3 = double(),
                method_1=double(),method_2=double(),
                method_3=double())

#define ranges to iterate

genres_n<-3:6
cluster_n <- 3:6
cutoff <- c(0.4,0.5,0.6)
iterations <-2:4


#iterate predictions
for(g in iterations){
  for(h in cutoff){
    for (i in genres_n){
      for(j in cluster_n){
 
        #create groups, biases  and predict
        
        user_classification  <- User_Classifier(edx_train_test$train,
                                                user_vector,
                                                genres=i,
                                                step_cutoff=h,
                                                cluster_n=j,
                                                cluster_type="GMM",
                                                iterations=g)
        
        biases_by_group <- Group_Biases(edx_train_test$train,
                                        user_classification)
        
        results<-Rating_Predicter(edx_train_test$test,user_classification,
                                  biases_by_group,general_biases)
        
        # store results in stats tibble
        stats <- add_row(stats,clustering="GMM",iterations=g,
                         cutoff=h,genres=i,cluster_n=j,
                         RMSE = results$RMSE$overall,
                         RMSE_1=results$RMSE$`1`,RMSE_2=results$RMSE$`2`,
                         RMSE_3=results$RMSE$`3`,
                         method_1=results$distribution_percentage$`1`,
                         method_2=results$distribution_percentage$`2`,
                         method_3=results$distribution_percentage$`3`)
     }
   }
 }
}
#cleanup
clustering_stats <- stats
rm(g,h,i,j,results,stats)

```

This code generates the results represented in the below chart.

```{r RMSE Graph, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
clustering_stats%>% ggplot(aes(x=cluster_n,y=RMSE,color=genres)) + geom_point() + facet_grid(.~iterations) +theme(legend.position="bottom") + labs(title="Model tuning by iterations",x="Clusters per iteration", y = "RMSE")+ geom_hline(yintercept=min(clustering_stats$RMSE), linetype="dashed", color = "red",size=0.5) + theme_grey()
```

The optimal solution has the below parameters,(also displaying the resulting overall RMSE plus the RMSE for the clustered (RMSE_1) and non-clusters (RMSE_2) groups, along with the proportion of reviews in each method).

```{r Clustering_Statistics, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}

clustering_stats[which.min(clustering_stats$RMSE),] %>% 
  select(-RMSE_3,method_3) %>%
  kable(caption="Optimal Clustering Parameters",format = "latex", booktabs = TRUE) %>%  kable_styling(latex_options = c("hold_position","scale_down"))
```

When compared with the *textbook* method, this procedure clearly a better RMSE:

```{r second_Prediction, eval=TRUE}

rmse_value <- min(clustering_stats$RMSE)

compare_RMSE <- add_row(compare_RMSE,
                        Model="Optimal Clusters",
                        RMSE=rmse_value,
                        "Below Target"=(rmse_value<target_rmse))

compare_RMSE %>% kable(caption="First and Second Predictions",format = "latex", booktabs = TRUE) %>%  kable_styling(latex_options = c("hold_position"),position = "center")

rm(rmse_value)
```

What are reasons this method has a better performance? Some explanations are:

* There are different averages for different groups, having more accurate stating points.
* Grouping provides movie biases with smaller deviations, contributing to a smaller RMSE. In this case,  `r mean((biases_by_group$movie_group_avgs %>% filter(!is.na(s_m)))$s_m<1)*100` % of the deviations are lower than 1, compared to `r mean((general_biases_first$movie_avgs %>% filter(!is.na(s_i)))$s_i<1)*100
` % for the *textbook* case. This can be visualised in the below chart:

```{r Movie_Biases_2, eval=TRUE}

h1 <- general_biases_first$movie_avgs %>% filter(!is.na(s_i)) %>%
  mutate(s=round(s_i,5),method="general") %>% select(s,method)
h2 <- biases_by_group$movie_group_avgs %>% filter(!is.na(s_m)) %>%
  mutate(s=round(s_m,5),method="clustering") %>% ungroup() %>% select(s,method)

h <-as_tibble(rbind(h1,h2))

ggplot(h,aes(x=s,y=stat(count / sum(count)))) + 
    geom_histogram(data=subset(h,method == 'general'),fill = "red", alpha = 0.2, label="General") +
    geom_histogram(data=subset(h,method == 'clustering'),fill = "blue", alpha = 0.4, label="Clustering") +
  theme(legend.position="bottom") + labs(title="Movie Biases's Std. Dev. Compared",x="Bias Standard Dev.", y = "Density")+ geom_vline(xintercept=1, linetype="dashed", color = "red",size=0.5) 

rm(h1,h2,h)
```

```{r Optimal_Clusters}

genres <- clustering_stats[which.min(clustering_stats$RMSE),]$genres
cluster_n <- clustering_stats[which.min(clustering_stats$RMSE),]$cluster_n
step_cutoff <- clustering_stats[which.min(clustering_stats$RMSE),]$cutoff
clustering_iterations <- clustering_stats[which.min(clustering_stats$RMSE),]$iterations
cluster_type <- "GMM"

user_classification  <- User_Classifier(edx_train_test$train,user_vector,
                                              genres=genres,step_cutoff=step_cutoff,
                                        cluster_n=cluster_n,cluster_type=cluster_type,
                                        iterations=clustering_iterations)

```

This solution splits the training set into `r length(unique(user_classification$group))` groups of users, with the below characteristics:

```{r user_group_stats}

group_stats<-edx_train_test$train %>% left_join(user_classification, by="userId") %>% group_by(group) %>% summarise(users=length(unique(userId)),movies=length(unique(movieId)))
group_stats

most_popular_movies <- tibble(group=double(),top_movies=character())

for(i in as.vector(group_stats$group)){
  temp <- (edx_train_test$train %>% left_join(user_classification, by="userId") %>%
             group_by(group,movieId, title) %>% summarise(reviews=n()) %>% filter(group==i) %>%
             arrange(-reviews) %>% ungroup()%>% select(-movieId))[1:5,]  %>%
             spread(title, reviews) %>% select(-group) %>% colnames(.)
  temp <- paste(temp, collapse = '; ',sep="; ")
  most_popular_movies<-add_row(most_popular_movies,group=i,top_movies=temp)
}

user_groups_stats <- group_stats %>% left_join(most_popular_movies, by="group")

rm(temp, most_popular_movies, group_stats)
```

```{r stats_table, eval=TRUE}
user_groups_stats %>% kable(caption="Optimal User Groups",format = "latex", booktabs = TRUE,format.args = list(decimal.mark = '.', big.mark = " ")) %>% 
  column_spec(4, width = "15em") %>%
  kable_styling(latex_options = c("HOLD_position"))

```

Looking at this and the compared deviation histogram's, it looks like there are groups that maybe are similar. Although left out of scope for this report, perhaps exploring better clustering mechanisms will deliver significantly improved results.

## Tuning Step 2: $\lambda$ parameters

Having settled on a set of optimal clustering parameters, there is still room to attempt improving the RMSE by tuning the $\lambda$ parameters in the movie and user bias equations:

$$b_i = \frac{1}{\lambda_1 + m}\sum_{j=1}^{m} (rating_j - \hat{\mu})$$
$$b_u = \frac{1}{\lambda_2 + n}\sum_{j=1}^{n} (rating_j - \hat{\mu} - b_{i,j})$$

In the proposed model, the are four lambda parameters: two for the clustering biases (from now on, $\lambda_1$ and $\lambda_2$) and two for the backstop model ($\lambda_3$ and $\lambda_4$). Given that the previous result shows that over 99% of the dataset fit in the cluster method, at this stage no tuning of ($\lambda_3$ and $\lambda_4$) will be done.

Additionally, it would be good to see how much the RMSE varies due to being a random variable, in order to have a level of confidence how well the method will perform against the validation set. To address this, k-fold cross validation will be conducted.

However, there is one problem:there are three variables to tune and each iteration requires data partition, user clustering, calculating biases and predictions. As per the experience from the previous optimisation step, it is safe to assume this will take **several** hours to complete. To alleviate this tuning will proceed the following way:

1. First, we will run iterations to run $lambda_1$ and $lambda_2$.
3. From this first set, we will choose the twenty combinations with the lowest RMSE.
4. Then, we will run k-fold validation for those twenty combinations.
5. Finally, the optimal set of $lambda_1$ and $lambda_2$ will be chosen picking the lowest average RMSE (from all k iterations).

The first iteration will be implemented with the below code:

```{r Lambda_Tuning_1, echo=TRUE}

stats <- tibble(k=double(),lambda_1=double(),lambda_2=double(),
                RMSE = double(),RMSE_1 = double(),RMSE_2 = double(),
                RMSE_3 =double(),
                method_1=double(),method_2=double(),method_3=double())

lambda_1 <- seq(0,8,0.5)
lambda_2 <- seq(0,8,0.5)


for(k in 1:1){

  edx_cross_validation <- Train_Test(edx_train_test$train)
              
  user_vector <-User_Vectoriser(edx_cross_validation$train)
  general_biases <- General_Biases(edx_cross_validation$train)
  user_classification  <- User_Classifier(edx_cross_validation$train,
                                          user_vector,
                                          genres=genres,
                                          step_cutoff=step_cutoff,
                                          cluster_n=cluster_n,
                                          cluster_type=cluster_type,
                                          iterations=clustering_iterations)  
  
  
  for (i in lambda_1){
    for(j in lambda_2){

              
              biases_by_group<-Group_Biases(edx_cross_validation$train,
                                            user_classification,
                                            lambda_1=i,lambda_2=j)
           
              results<-Rating_Predicter(edx_cross_validation$test,
                                        user_classification,
                                        biases_by_group,general_biases)
              
              stats <- add_row(stats, k=k, lambda_1=i,lambda_2=j,
                               RMSE = results$RMSE$overall,
                         RMSE_1=results$RMSE$`1`,
                         RMSE_2=results$RMSE$`2`,
                         RMSE_3=results$RMSE$`3`,
                         method_1=results$distribution_percentage$`1`,
                         method_2=results$distribution_percentage$`2`,
                         method_3=results$distribution_percentage$`3`)
  }
  }
  
  rm(edx_cross_validation)

}

group_lambda_stats_1 <- stats
rm(i,j,k)
```

The 20 lowest RMSE combinations are presented in the below table:

```{r Lambda_Tuning_2,eval=TRUE}

cases <- 20
group_lambda_stats_1<- unique(group_lambda_stats_1)

#lambda_aggregate_stats_1<-group_lambda_stats_1 %>% group_by(lambda_1,lambda_2) %>% summarise(Avg_RMSE=mean(RMSE),Avg_Method_1=mean(method_1))

lambda_top_20_rmse_1<-group_lambda_stats_1 %>% arrange(RMSE) %>% head(cases)    
lambda_top_20_rmse_1$cases <- seq.int(nrow(lambda_top_20_rmse_1))

lambda_top_20_rmse_1 %>% select(-k,-RMSE_3,-method_3,-cases) %>%
  kable(caption="Lambda Optimisation - Step 1",format = "latex", booktabs = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position","scale_down"))


```

Then, using a modified version of the previous code, 5 more iterations of those 20 sets of $lambdas$ are generated and average RMSEs are calculated. The best five results are presented in the below table.

```{r Lambda_Tuning_3}

stats <- tibble(k=double(), cases=double(),lambda_1=double(),lambda_2=double(),
                RMSE = double(),RMSE_1 = double(),RMSE_2 = double(),
                RMSE_3 =double(),
                method_1=double(),method_2=double(),method_3=double())

lambda_1 <- seq(0,8,0.5)
lambda_2 <- seq(0,8,0.5)

for(k in 1:6){

  edx_cross_validation <- Train_Test(edx_train_test$train)
              
  user_vector <-User_Vectoriser(edx_cross_validation$train)
  general_biases <- General_Biases(edx_cross_validation$train)
  user_classification  <- User_Classifier(edx_cross_validation$train,user_vector,
                                              genres=genres,step_cutoff=step_cutoff,
                                        cluster_n=cluster_n,cluster_type=cluster_type,
                                        iterations=clustering_iterations)  
  
  
  for (i in 1:cases){
  

              
              biases_by_group <- Group_Biases(edx_cross_validation$train,user_classification,
                                              lambda_1=lambda_top_20_rmse_1[i,]$lambda_1,
                                              lambda_2=lambda_top_20_rmse_1[i,]$lambda_2)
           
              results<-Rating_Predicter(edx_cross_validation$test,user_classification,biases_by_group,general_biases)
              
              stats <- add_row(stats, k=k, cases=i,lambda_1=lambda_top_20_rmse_1[i,]$lambda_1,
                               lambda_2=lambda_top_20_rmse_1[i,]$lambda_2,
                               RMSE = results$RMSE$overall,
                         RMSE_1=results$RMSE$`1`,RMSE_2=results$RMSE$`2`,RMSE_3=results$RMSE$`3`,
                         method_1=results$distribution_percentage$`1`,method_2=results$distribution_percentage$`2`,
                         method_3=results$distribution_percentage$`3`)
  }
  
  
  rm(edx_cross_validation)

}

group_lambda_stats <- stats
rm(i,j,k)
   
```

```{r Lambda_Tuning_4, eval=TRUE}

group_lambda_stats <-rbind(group_lambda_stats,lambda_top_20_rmse_1)
group_lambda_stats<- unique(group_lambda_stats)

lambda_aggregate_stats<- group_lambda_stats %>% group_by(cases,lambda_1,lambda_2) %>% summarise(Avg_RMSE=mean(RMSE)) %>%ungroup()

lambda_top_rmse<-lambda_aggregate_stats %>% arrange(Avg_RMSE) %>% head(5)    
             
lambda_top_rmse  %>% select(-cases) %>%
  kable(caption="Lambda Optimisation - Step 2",format = "latex", booktabs = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position"))

```

Using these parameters, the model is retrained using the whole training set and RMSE is calculated against the test set for comparison with the previous two cases.

```{r Retrain_Optimal_Lambdas}
lambda_1<-lambda_top_rmse[which.min(lambda_top_rmse$Avg_RMSE),]$lambda_1
lambda_2<-lambda_top_rmse[which.min(lambda_top_rmse$Avg_RMSE),]$lambda_2

user_vector <-User_Vectoriser(edx_train_test$train)
general_biases <- General_Biases(edx_train_test$train)
user_classification  <- User_Classifier(edx_train_test$train,user_vector,
                                              genres=genres,step_cutoff=step_cutoff,
                                        cluster_n=cluster_n,cluster_type=cluster_type,
                                        iterations=clustering_iterations)
biases_by_group <- Group_Biases(edx_train_test$train,user_classification,lambda_1 = lambda_1,lambda_2 = lambda_2)
results<-Rating_Predicter(edx_train_test$test,user_classification,biases_by_group,general_biases)

stats <- tibble(lambda_1=lambda_1,
                lambda_2=lambda_2,
                RMSE = results$RMSE$overall,
                RMSE_1=results$RMSE$`1`,
                RMSE_2=results$RMSE$`2`,
                method_1=results$distribution_percentage$`1`,
                method_2=results$distribution_percentage$`2`)

optimal_lambdas <- stats
results_test <- results
rm(results,stats)
```

```{r Assess_Test_Set, eval=TRUE }

optimal_lambdas %>%
  kable(caption="Result with Optimal Lambdas on Test Set",format = "latex", booktabs = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position"))
          
```

```{r third_Prediction, eval=TRUE}

rmse_value <- min(optimal_lambdas$RMSE)

compare_RMSE <- add_row(compare_RMSE,
                        Model="Optimal Clusters + Optimised Lambdas",
                        RMSE=rmse_value,
                        "Below Target"=(rmse_value<target_rmse))

compare_RMSE %>% kable(caption="All Training Predictions Compared",format = "latex", booktabs = TRUE) %>%  kable_styling(latex_options = c("HOLD_position"),position = "center")

rm(rmse_value)
```

## Checks prior to prediction

Before moving to assess the model against the validation set, there are two points to look at. The first point (as mentioned before) is the RMSE variance when k-fold validation was done. Taking the 5 best results, it is possible to build the below box plot, which shows all combinations well below the RMSE threshold.

```{r RMSE_variation}
group_lambda_stats %>% filter(cases %in% lambda_top_rmse$cases) %>% 
  mutate(l1l2=paste(lambda_1,lambda_2,sep=", ")) %>%
  select(l1l2,RMSE) %>%
  ggplot(aes(x=l1l2,y=RMSE,color=l1l2)) + geom_boxplot() + theme(legend.position="bottom") + labs(title="RMSE variation due k-fold validation",x="lambda_1, lambda2", y = "RMSE")+ geom_hline(yintercept=target_rmse, linetype="dashed", color = "red",size=0.5) + theme_grey()
```

The second inspection is to have a look at the films that couldn't be rated by the clustering mechanism. The below table shows the top 20 films in that category. After reviewing this list, those films seem to be mostly obscure and thus seems reasonable that couldn't be found in the clustering movie biases. Even if the user is not new, this result would be expected for rare films.

```{r Analysis_2, eval=TRUE}
### Analyse test results, comments


(results_test$prediction %>% filter(method==2) %>% 
   group_by(title) %>% summarise(n=n()) %>% arrange(-n))[1:20,] %>%
  kable(caption="Non Clusterable Reviews - Movies",format = "latex", booktabs = TRUE) %>%  kable_styling(latex_options = c("HOLD_position"),position = "center")


```

# Results
<!-- a results section that presents the modelling results and discusses the model performance -->

With all optimised parameters, it is time now to try to predict the ratings for the validation set. This has been done in two parts:

* First, the model has been be re-trained using all optimal parameters and the whole combined training+test dataset.
* Then, the model will be used to predict the ratings for the validation set and the RMSE will be calculated.

The code to achieve this is presented below.

```{r Validation_Prediction, echo=TRUE}
# Prediction against validation dataset

##re-join training test tests
edx <- rbind(edx_train_test$train,edx_train_test$test)

### Retrain : reclustering and re-calculation of biases with new dataset

user_vector_prediction <-User_Vectoriser(edx)
general_biases_prediction <- General_Biases(edx)
user_classification_prediction  <- User_Classifier(edx,user_vector_prediction,
                                              genres=genres,
                                              step_cutoff=step_cutoff,
                                              cluster_n=cluster_n,
                                              cluster_type=cluster_type,
                                              iterations=clustering_iterations)
biases_by_group_prediction <- Group_Biases(edx,
                                           user_classification_prediction,
                                           lambda_1=lambda_1,
                                           lambda_2=lambda_2)

### tidy up validation table

validation<-Tidy_Up(validation)

# Predict ratings

prediction <- Rating_Predicter(validation,
                               user_classification_prediction,
                               biases_by_group_prediction,
                               general_biases_prediction)


##Calculate method split and RMSE

stats <- tibble(RMSE = double(),RMSE_1 = double(),
                RMSE_2 = double(),
                method_1=double(),method_2=double())

stats <- tibble(RMSE = prediction$RMSE$overall,
                         RMSE_1=prediction$RMSE$`1`,
                         RMSE_2=prediction$RMSE$`2`,
                         method_1=prediction$distribution_percentage$`1`,
                         method_2=prediction$distribution_percentage$`2`)
prediction_stats <-stats

rm("stats")         
```

The RMSE results are shown in the following table:

```{r Prediction_Stats, eval=TRUE}
prediction_stats %>% select(-RMSE_3,-method_3) %>%
  kable(caption="RMSE for validation set",format = "latex", booktabs = TRUE) %>% 
  kable_styling(latex_options = c("HOLD_position"),position = "center")
```

In terms of meeting the RMSE requirements, this model has met the objective. However, as mentioned and hinted through this report, the following limitations have been discovered:

* First, the time required to fit and tune the model is considerable. On a laptop, this can take up a day or more, which makes the whole process slow to run and even slower if it requires multiple test and repetitions to find the optimal results.
* Second, the issues regarding user variability have not been addressed (e.g. big variance of rating for one user). This is an area of improvement that perhaps will further improve the model.

Having said that, this model is still relative simple and fast for predictions, which is a big advantage - future work may still want to leverage this feature, through further modifications or combination with other methods.

\newpage
# Conclusion
<!-- a conclusion section that gives a brief summary of the report, its limitations and future work -->

In conclusion, this report contains the development of a recommendation system, based on the one presented in the course's textbook. From this starting point, the model was improved by segmenting users into groups. Through this re-fitting and further tuning, a RMSE of `r compare_RMSE %>% filter(Model=="Optimal Clusters + Optimised Lambdas") %>% pull(RMSE)` was obtained. This results exceeds the goal of achieving a value lower than `r target_rmse`.

As mentioned in the previous sections, despite its simplicity the resulting model is time-intensive when it comes to fitting. This is mainly due to the clustering process, which potentially requires multiple iterations. Although it provides an improvement to the previous model, there are still many individual cases where the error is greater than acceptable to provide a reliable prediction of whether a user will like or dislike a particular movie.

Of course, this model can be improved if further work is undertaken, namely:

* Assess if $\lambda$ parameters per user cluster/movie improve the results vs. the extra time it will take to fit the model.
* Cluster films by common characteristics (genre but also perhaps actor,director, etc. ) and see if it is worth to use that to provide more accurate user biases.
* Explore other clustering methods, assessing both changes in RMSE and fitting effort.

In addition to the above, but perhaps out of scope in the context of this exercise (due to the way the validation set is created), it would be interesting to explore if there is an  optimal dataset size. For instance, if we consider the that user cinematic preferences change over time, perhaps the oldest reviews are not as important as the ones from last year. Therefore, this opens the opportunity to treat the biases as  function of time, or simply discard older reviews and have a leaner dataset resulting in faster processing times without a significant loss of accurary.
